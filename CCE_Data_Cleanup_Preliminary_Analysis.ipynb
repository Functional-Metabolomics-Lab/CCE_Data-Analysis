{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e139f23e",
   "metadata": {},
   "source": [
    "### CCE Data - Preliminary data cleanup and MDS plots  \n",
    "Authors: Abzer Kelminal (abzer.shah@uni-tuebingen.de) <br>\n",
    "Edited by:    <br>\n",
    "Input file format: .csv files directly from github <br> \n",
    "Outputs: .csv files and .svg files(plots) <br>\n",
    "Dependencies: ggplot2, dplyr, ecodist, RcolorBrewer, svglite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the needed packages:\n",
    "\n",
    "#install.packages(\"dplyr\")\n",
    "#install.packages(\"ggplot2\")\n",
    "#install.packages(\"ecodist\")  #For PcoA using Bray-Curtis distance\n",
    "#install.packages(\"RColorBrewer\") # to use Colorblindness-friendly colors\n",
    "#install.packages(\"svglite\") # for saving ggplots as svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the libraries:\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(ecodist) \n",
    "library(RColorBrewer)\n",
    "library(svglite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d92454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly calling the files from the github account:\n",
    "ft_url <- \"https://raw.githubusercontent.com/Functional-Metabolomics-Lab/CCE_Data-Analysis/main/raw%20files/CCE1706_MZmine3_GNPS_1_quant.csv\"\n",
    "md_url <- \"https://raw.githubusercontent.com/Functional-Metabolomics-Lab/CCE_Data-Analysis/main/raw%20files/metadata_CCE.csv\"\n",
    "\n",
    "ft <- read.csv(ft_url, header = T, check.names = F)\n",
    "md <- read.csv(md_url, header = T, check.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d71ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9302fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the metadata table in the right format for further analysis:\n",
    "\n",
    "md <- md[,colSums(is.na(md))<nrow(md)] #removing NA columns in md, if any present\n",
    "rownames(md) <- md$filename \n",
    "md <- md[,-1]\n",
    "rownames(md) <- gsub('Blank_','',rownames(md)) # Removing \"Blank\" in the 1st two row names of md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecb232",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the feature table in the right format for further analysis:\n",
    "\n",
    "new_ft <- ft[,grep('mzXML',colnames(ft))] # only picking mzXML files\n",
    "new_ft <- new_ft[,-grep('_STKL',colnames(new_ft))] # excluding the STKL columns\n",
    "colnames(new_ft) <- gsub('_MSMS.mzXML Peak area','.mzxml',colnames(new_ft))  #substituting the file extension in the colnames of new_ft with mzxml\n",
    "rownames(new_ft) <- paste(ft$'row ID',round(ft$'row m/z',digits = 3),round(ft$'row retention time',digits = 3), sep = '_') #Taking row ID, m/z value and RT of ft as the rownames of new_ft\n",
    "\n",
    "new_ft <- new_ft[,order(colnames(new_ft))] #ordering the columns by names\n",
    "new_ft <- new_ft[,-1] # Removing the column \"Brandon_Deep_DOM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eca56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(new_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a775a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_final <- new_ft[,which(colnames(new_ft)%in%rownames(md))] #picking only the columns in new_ft that are comparable to rownames of md\n",
    "ft_final  <- new_ft[,match(rownames(md),colnames(new_ft))] #returns the matched position of rownames of md to that of colnames of new_ft\n",
    "identical(colnames(ft_final),rownames(md)) #checking if the colnames of ft_final and rownames of md matched. Should return TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(ft_final)\n",
    "dim(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1c70a5",
   "metadata": {},
   "source": [
    "The dimensions show that nrow(ft_final)=ncol(md), thus eligible for matrix multiplication. Therefore, we can apply several calculations on these tables such as PCA etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96275453",
   "metadata": {},
   "source": [
    "### Creating a function named FrequencyPlot:  \n",
    "The below function takes in the two input datatables: gapfilled and non-gapfilled, calculates the frequency distribution of the data in the order of 10 and produces a grouped barplot showing the distribution as output. The frequency plot shows where the features are present in higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6826f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=3) #'global' settings for plot size in the output cell\n",
    "\n",
    "FrequencyPlot <- function(x1,x2){\n",
    "    \n",
    "    #creating bins from -1 to 10^10 using sequence function seq()\n",
    "    bins <- c(-1,0,(1 * 10^(seq(0,10,1)))) \n",
    "    \n",
    "    #cut function cuts the give table into its appropriate bins\n",
    "    scores_x1 <- cut(as.matrix(x1),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10')) \n",
    "    \n",
    "    #transform function convert the tables into a column format: easy for visualization \n",
    "    Table_x1<-transform(table(scores_x1)) #contains 2 columns: \"scores_x1\", \"Freq\"\n",
    "    \n",
    "    #Repeating the same steps for x2\n",
    "    scores_x2 <- cut(as.matrix(x2),bins,labels = c('0','1','10','1E2','1E3','1E4','1E5','1E6','1E7','1E8','1E9','1E10'))\n",
    "    Table_x2<-transform(table(scores_x2))\n",
    "  \n",
    "    #Getting the names of x1 and x2\n",
    "    arg1 <- deparse(substitute(x1))\n",
    "    arg2 <-deparse(substitute(x2))\n",
    "    \n",
    "    #Creating a data frame for plotting\n",
    "    data_plot <- as.data.frame(c(Table_x1$Freq,Table_x2$Freq)) #Concatenating the frequency info of both tables rowwise\n",
    "    colnames(data_plot) <- \"Freq\" #naming the 1st column as 'Freq'\n",
    "    data_plot$Condition <- c(rep(arg1,12),rep(arg2,12)) #adding a 2nd column 'Condition', which just repeats the name of x1 and x2 accordingly\n",
    "    data_plot$Range_bins <- rep(Table_x1$scores_x1,2) #Adding 3rd column 'Range Bins'\n",
    "    data_plot$Log_Freq <- log(data_plot$Freq+1) #Log scaling the frequency values\n",
    "    \n",
    "    ## GGPLOT2\n",
    "    BarPlot <- ggplot(data_plot, aes(Range_bins, Log_Freq, fill = Condition)) + \n",
    "    geom_bar(stat=\"identity\", position = \"dodge\", width=0.4) + \n",
    "    scale_fill_brewer(palette = \"Set1\") +\n",
    "    ggtitle(label=\"Frequency plot\") +\n",
    "    xlab(\"Range\") + ylab(\"(Log)Frequency\") + labs(fill = \"Data Type\") + \n",
    "    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +   # setting the angle for the x label\n",
    "    theme(axis.text.y = element_text(angle = 45, vjust = 0.5, hjust=1)) +   # setting the angle for the y label\n",
    "    theme(plot.title = element_text(hjust = 0.5)) # centering the plot title\n",
    "  \n",
    "    print(BarPlot)\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a739505",
   "metadata": {},
   "source": [
    "## Blank Removal:\n",
    "\n",
    "(Note: In LC-MS/MS, we use solvents also called as Blanks which are usually injected time-to-time to prevent carryover of the sample)\n",
    "\n",
    "The blanks we are referring to here, is the control blanks in the experiment and not the LCMSMS blanks.\n",
    "- The control blanks here is the sample without treatment. \n",
    "- Samples are biological replicates with some treatment \n",
    "\n",
    "For the next step, Blank removal, we need to split the data as control and samples.\n",
    "\n",
    "In general, having multiple control blanks helps us to compare any variation in the data. Comparing control to the sample helps us to identify the background features that contribute to any technical variation.  \n",
    "A common filtering method is to use a cutoff to remove features that are not present sufficient enough in biological samples.\n",
    "\n",
    "1. We find an average for all the feature intensities in your control set and sample set.\n",
    "Therefore, for n no.of features in a control or sample set, we get n no.of averaged features.\n",
    "2. Next, we get a ratio of this average_control vs average_sample. This ratio Control/sample tells us how much of that particular feature of a sample gets its contribution from control. If it is more than 30% (or Cutoff as 0.3), we consider the feature as noise.\n",
    "3. The resultant information (if ratio > Cutoff or not) is stored in a bin\n",
    "4. We count the no.of features in the bin that satisfies the condition ratio > cutoff, and consider those features as 'noise or background features'.\n",
    "5. We also try to visualize the frequency distribution in our data Ctrl and samples\n",
    "\n",
    "For a dataset containing several batches, the filtering steps are performed batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the samples: \n",
    "Ctrl <-  ft_final[,1:2]\n",
    "Samples <- ft_final[,-1:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b25c",
   "metadata": {},
   "source": [
    "For the code below, <font color='red'> I selected 'Y' and gave a cutoff value: 0.3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7055b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data <- ft_final\n",
    "\n",
    "if(readline('Do you want to perform Blank Removal Process - Y/N:')=='Y'){\n",
    "  Cutoff <- as.numeric(readline('Enter Cutoff value between 0.1 & 1:')) # (i.e. 10% - 100%). Ideal cutoff range: 0.1-0.3\n",
    "  #When cutoff is low, more noise (or background) detected; With higher cutoff, less background detected, thus more features observed\n",
    "  \n",
    "  Avg_ctrl <- rowMeans(Ctrl, na.rm= FALSE, dims = 1) # set na.rm = FALSE to check if there are NA values. Because when set as TRUE, NA values are changed to 0\n",
    "  Avg_samples <- rowMeans(Samples, na.rm= FALSE, dims = 1)\n",
    "  Ratio_Ctrl_Sample <- (Avg_ctrl+1)/(Avg_samples+1)\n",
    "  Bg_bin <- ifelse(Ratio_Ctrl_Sample > Cutoff, 1, 0 )  # checks if the Ratio is greater than Cutoff, if so put 1, else 0 in Bg_bin\n",
    "  \n",
    "  # to check if there are any NA values present. It is not good to have NA values in the 4 variables as it will affect the final dataset to be created\n",
    "  temp_NA_Count <-cbind(Avg_ctrl ,Avg_samples,Ratio_Ctrl_Sample,Bg_bin)\n",
    "  \n",
    "  print('No of NA values in the following columns:')\n",
    "  print(colSums(is.na(temp_NA_Count)))\n",
    "  \n",
    "  Bg_Features <- sum(Bg_bin ==1,na.rm = TRUE) # Calculates the number of background features present\n",
    "  No_of_Features <- nrow(input_data) - Bg_Features\n",
    "  \n",
    "  print(paste(\"No.of Background or noise features:\",Bg_Features))\n",
    "  print(paste(\"No.of features after excluding noise:\",No_of_Features)) \n",
    "  \n",
    "  input_data1 <- cbind(as.matrix(input_data),Bg_bin)    \n",
    "  plot_CtrlSample <- FrequencyPlot(Samples,Ctrl)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08bb0b",
   "metadata": {},
   "source": [
    "## Imputation:\n",
    "For several reasons, real world datasets might have some missing values in it, in the form of **NA, NANs or 0s**. Eventhough the gapfillig step of MZmine fills the missing values, we still end up with some missing values or Os in our feature table. This could be problematic for statistical analysis. In order to have a better dataset, we cannot simply discard those rows or columns with missing values as we will lose a chunk of our valuable data. Instead we can try imputing those missing values. Imputation involves replacing the missing values in the data with a meaningful, reasonable guess. There are several methods, such as:\n",
    "\n",
    "- Mean imputation (replacing the missing values in a column with the mean or average of the column)\n",
    "- Replacing it with the most frequent value\n",
    "- Several other machine learning imputation methods such as k-nearest neighbors algorithm(k-NN), Hidden Markov Model(HMM)\n",
    "\n",
    "The method that we use is: Replacing the zeros from the gapfilled quant table with the Cutoff_LOD value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ca879",
   "metadata": {},
   "source": [
    "### Continuing from previous steps in Blank Removal :  \n",
    "6. **From the plot, we decide on a cutoff_LOD value (LOD-Limit of Detection).** If until range 10-100, (shown in the figure as 1E2) there are no or very less features, we want to exclude until that range and consider from range (100-1000), or, in other words, we take 'range:100-1000 or 1E3 or 1000' as Cutoff_LOD. This value will be used to replace the zeros in the data table\n",
    "7. Once we consider that LOD value,\n",
    "    - We create  a temparory dataset with all the feature intensites of your sample (not the ctrl, only the sample) and checking it against the cutoff_LOD value. If it is less than the cutoff_LOD, we replace it with cutoff_LOD. Thus, for instance, if we take 1000 as cutoff_LOD, our sample data will be filled with a bunch of 1000s now instead of zeros\n",
    "    - Then we create a Final dataset using the temporary dataset. Here, we try to see if each feature from all samples is noise or not (from step 4), if it noise, we replace the feature with cutoff_LOD, else we keep the info from the temporary dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3de2a9",
   "metadata": {},
   "source": [
    "For the code below, <font color='red'> I selected 'N' and gave a LOD value: 1000 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec344af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the LOD value as seen in the frequency plot: Ex: 1E3 or 1000\n",
    "Cutoff_LOD <-ifelse(readline(\"Was Imputation step already performed? Y/N\")==\"Y\",RawLOD,as.numeric(readline(\"Enter your Cutoff LOD here:\")))  \n",
    "\n",
    "# Replacing the Sample intensities with Cutoff_LOD, if they are lower than Cutoff_LOD\n",
    "temp_matrix <- c()\n",
    "for (i in 1:ncol(Samples)){ \n",
    "    x <- ifelse(Samples[,i] > Cutoff_LOD, Samples[,i],Cutoff_LOD)\n",
    "    temp_matrix <- cbind(temp_matrix,as.matrix(x))\n",
    "  }\n",
    "colnames(temp_matrix) <- colnames(Samples)\n",
    "\n",
    "# Replacing the Sample intensities with Cutoff_LOD, if they are considered as a noise(bg_bin==1)\n",
    "Final_matrix <-c()\n",
    "for (i in 1:ncol(temp_matrix)){\n",
    "    x <-ifelse(input_data1[,ncol(input_data1)] ==1, Cutoff_LOD, temp_matrix[,i])\n",
    "    Final_matrix <-cbind(Final_matrix,x)\n",
    "  }\n",
    "colnames(Final_matrix) <- colnames(Samples)\n",
    "\n",
    "\n",
    "write.csv(Final_matrix,file=paste0('Processed_Quant_Table_filled_with_Value_',Cutoff_LOD ,'.csv'),row.names =FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a04d79",
   "metadata": {},
   "source": [
    "## Normalization:\n",
    "The following code performs sample-centric (column-wise) normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample centric normalisation:----------------------------------------------- I said \"Y\" (YES) here\n",
    "if (readline(\"Do you want to perform Normalization: Y/N:\") == 'Y'){  \n",
    "  sample_sum <- colSums(Final_matrix, na.rm= TRUE, dims = 1)\n",
    "  Normalized_data <- c()\n",
    "  for (i in 1:ncol(Final_matrix)){\n",
    "    x <- Final_matrix[,i] / sample_sum[i]\n",
    "    Normalized_data <- cbind(Normalized_data, x)\n",
    "  }\n",
    "  \n",
    "  colnames(Normalized_data) <- names(sample_sum)\n",
    "  \n",
    "  \n",
    "} else return(Final_matrix)\n",
    "\n",
    "print(paste('No.of NA values in Normalized data:',sum(is.na(Normalized_data)== TRUE)))\n",
    "#write.csv(Normalized_data,file=paste0('CCE_Normalised_Data' ,'.csv'),row.names =FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709bf061",
   "metadata": {},
   "source": [
    "## MDS Plots\n",
    "The goal of Multi-Dimensional scaling (MDS) is to visualize multivariate data in 2D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluding Blanks from md and some other filtering conditions\n",
    "MetaData <-  md %>% filter(ATTRIBUTE_Filament_Possition != 'Blank',\n",
    "                           ATTRIBUTE_Location!='Santa_Barbara_Basin',\n",
    "                           ATTRIBUTE_Location!='Transcet_1',\n",
    "                           ATTRIBUTE_Location!='Transcet_2',\n",
    "                           ATTRIBUTE_Location!='Transect_3',\n",
    "                           ATTRIBUTE_Depth <= 20 ) \n",
    "\n",
    "# the corresponding column files for the filtered MetaData is picked from the normalized data\n",
    "md_data <- Normalized_data[,which(colnames(Normalized_data)%in%rownames(MetaData))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(MetaData)\n",
    "dim(MetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(md_data)\n",
    "dim(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089ced49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For PCA plots, use Euclidean distance:\n",
    "#dist_matrix <- dist(t(md_data), method=\"euclidean\")\n",
    "\n",
    "#For PCoA plots using Bray-curtis distance:\n",
    "dist_matrix <- bcdist(t(md_data)) # transposed in order to compute the distance between the columns of a data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcoa<- cmdscale(dist_matrix, eig = TRUE, x.ret=TRUE)\n",
    "pcoa.var.per <-round(pcoa$eig/sum(pcoa$eig)*100,1)\n",
    "pcoa.values <- pcoa$points\n",
    "pcoa.data <- data.frame(MetaData$ATTRIBUTE_Depth_Range,\n",
    "                        X=pcoa.values[,1],\n",
    "                        Y=pcoa.values[,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GGPLOT\n",
    "Plot <- ggplot(pcoa.data, aes(x=X, y=Y, col= as.factor(MetaData$ATTRIBUTE_Filament_Possition))) + \n",
    "  #geom_jitter(aes(shape = as.factor(MetaData$ATTRIBUTE_Depth)), size = 3) +\n",
    "  geom_point(size=2,alpha=0.8)  +\n",
    "  ggtitle(label=\"MDS plot\") +\n",
    "  xlab(paste0(\"MDS1 : \",pcoa.var.per[1],\"%\",sep=\"\")) + \n",
    "  ylab(paste0(\"MDS2 : \",pcoa.var.per[2],\"%\",sep=\"\")) + \n",
    "  labs(color = \"Cycles\",shape='Depth Range') + \n",
    "  theme(plot.title = element_text(hjust = 0.5)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0975adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot <- Plot + labs(subtitle = 'Using Bray_Curtis Distance on normalized data')\n",
    "Plot\n",
    "#ggsave(file=\"PCoA_Depth_0_20.svg\", plot=Plot, width=10, height=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To visualize the color palette\n",
    "\n",
    "#options(repr.plot.width=5, repr.plot.height=6) #'global' settings for plot size in the output cell\n",
    "#display.brewer.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0bf4c4",
   "metadata": {},
   "source": [
    "In the below code, n=5 is selected as default. Because, in general, the palette breaks the color from darker shade to lighter. In order to avoid considering really light colors in the plot (as it is hard to see), we specify a random n=5 as it is greater than no.of days (Max no.of days in a cycle=4) and take the first few colors according to the no.of days in each cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9beaf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 <- rev(brewer.pal(5, \"YlOrRd\"))\n",
    "b1 <- rev(brewer.pal(5, \"PuBu\"))\n",
    "c1 <- rev(brewer.pal(5, \"Greens\"))\n",
    "d1 <- rev(brewer.pal(5, \"RdPu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rep(1,length(b1)),col= b1,pch=19,cex=3,ylab=NULL,xlab=NULL) #For visualising the gradient colors in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19014144",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 <- a1[1:length(grep(Primary_level[1],levels(col_used)))]\n",
    "b1 <- b1[1:length(grep(Primary_level[2],levels(col_used)))]\n",
    "c1 <- c1[1:length(grep(Primary_level[3],levels(col_used)))]\n",
    "d1 <- d1[1:length(grep(Primary_level[4],levels(col_used)))]\n",
    "\n",
    "colors_gradient <- c(a1,b1,c1,d1)\n",
    "colors_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b75ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rep(1,length(b1)),col= b1,pch=19,cex=3,ylab=NULL,xlab=NULL) #For visualising the gradient colors in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff36160",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot2 <- ggplot(pcoa.data, aes(x=X, y=Y, col= as.factor(MetaData$ATTRIBUTE_Location))) + \n",
    "  geom_point(size=2,alpha=0.8)  +\n",
    "  ggtitle(label=\"MDS plot\") +\n",
    "  xlab(paste0(\"MDS1 : \",pcoa.var.per[1],\"%\",sep=\"\")) + \n",
    "  ylab(paste0(\"MDS2 : \",pcoa.var.per[2],\"%\",sep=\"\")) + \n",
    "  labs(color = \"Location\") + \n",
    "  theme(plot.title = element_text(hjust = 0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d7fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot2 <- Plot2 + scale_color_manual(values = colors_gradient)+\n",
    "  labs(subtitle = 'Using Bray_Curtis Distance on normalized data')\n",
    "\n",
    "Plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save a ggplot as svg (scaled vector graph)\n",
    "ggsave(file=\"PCoA_Depth_0_20.svg\", plot=Plot2, width=10, height=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
